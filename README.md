# <div align="center">Talk</div>

<div align="center">
<strong>100% Local, AI-Powered Chat & Face Fusion Platform</strong>
</div>

<br />

<div align="center">

![React](https://img.shields.io/badge/React-18.2-61dafb?style=for-the-badge&logo=react&logoColor=white)
![FastAPI](https://img.shields.io/badge/FastAPI-0.109-009688?style=for-the-badge&logo=fastapi&logoColor=white)
![Python](https://img.shields.io/badge/Python-3.11-3776AB?style=for-the-badge&logo=python&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)

<br />

<a href="#">
<img src="https://img.shields.io/badge/Status-Active-success?style=for-the-badge" height="50" />
</a>

<br />
<br />

**[ğŸš€ Local AI Powerhouse running on Apple Silicon]**

</div>

<br />

> **"Privacy-first AI that lives on your Mac."**
>
> Talk isn't just a chatbot; it's a complete local AI suite.  
> Powered by Llama 3, InsightFace, and GFPGAN, it brings state-of-the-art LLM chat and Face Manipulation tools directly to your desktop, offline and secure.

---

## ğŸŒŸ Vision

Talk's mission is to be:

- **A completely local AI platform** â€” no data leaves your device
- **A multi-modal powerhouse** â€” Text, Images, PDFs, and Face Swapping
- **A beautiful, modern web application** â€” Apple-inspired "Liquid Glass" design

---

## âœ¨ Why Talk?

Most AI tools require cloud subscriptions and sacrifice privacy.  
Talk democratizes AI by running **Llama 3 (Chat)** and **FaceFusion (Deepfakes)** entirely on your M1/M2/M3 Mac with Metal acceleration.

---

## ğŸ¨ Apple-Inspired "Liquid Glass" Design

- **Minimalist Aesthetics**  
  Pure CSS implementation following Apple's design principles â€” no frameworks, just elegance.

- **Liquid Glass Effects**  
  Translucent overlays with `backdrop-filter: blur()` create depth and focus.

- **Soft Elevation**  
  Subtle shadows and smooth transitions provide a premium feel.

- **System Fonts**  
  Native `-apple-system` typography for maximum legibility and native feel.

---

## ğŸ¤– AI-Powered Intelligence

- **Local Llama 3 Chat**  
  Run Meta's latest 8B model locally with `llama.cpp` and Metal acceleration.

- **RAG Pipeline**  
  Chat with your PDF documents and images using FAISS vector search and OCR.

- **Face Lab (Face Swap)**  
  Swap faces in Images and Videos using `inswapper_128` and `insightface`.

- **High-Definition Restoration**  
  Automatically upscale and restore swapped faces to 1080p quality using **GFPGAN**.

- **WAN Analysis**  
  Deep facial analysis to detect age, gender, and pose.

---

## ğŸ“ Project Structure

```
Talk/
â”œâ”€â”€ backend/                      # FastAPI Python Server
â”‚   â”œâ”€â”€ app.py                    # Main entry point
â”‚   â”œâ”€â”€ model_runner.py           # Llama 3 Wrapper (llama-cpp-python)
â”‚   â”œâ”€â”€ embeddings.py             # SentenceTransformers
â”‚   â”œâ”€â”€ vectordb.py               # FAISS Vector Database
â”‚   â”œâ”€â”€ facefusion/               # Face Swap Logic
â”‚   â”‚   â””â”€â”€ fusion.py             # InsightFace + GFPGAN Wrapper
â”‚   â”œâ”€â”€ routers/                  # API Routes
â”‚   â”‚   â””â”€â”€ face_router.py        # Face Swap & Analysis Endpoints
â”‚   â””â”€â”€ models/                   # Local Model Weights (.gguf, .onnx, .pth)
â”‚
â””â”€â”€ frontend/                     # React + Vite SPA
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ App.jsx               # Main Layout
    â”‚   â”œâ”€â”€ Chat.jsx              # Chat Interface (Streaming)
    â”‚   â”œâ”€â”€ FaceLab.jsx           # Face Swap UI (Image/Video)
    â”‚   â”œâ”€â”€ WanAnalysis.jsx       # Face Analysis UI
    â”‚   â”œâ”€â”€ Sidebar.jsx           # Navigation
    â”‚   â””â”€â”€ styles.css            # Pure CSS (Apple-inspired)
    â””â”€â”€ index.html
```

---

## ğŸš€ Quick Start

### Prerequisites

- **Mac with Apple Silicon** (M1/M2/M3)
- **Node.js** (v18+)
- **Python** (3.10+)

### 1. Clone the Repository

```bash
git clone https://github.com/zakisheriff/Talk.git
cd Talk
```

### 2. Install Dependencies

```bash
# Backend
cd backend
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Frontend
cd ../frontend
npm install
```

### 3. Download Models

The system requires several large model files (~4GB total).
Run the setup script or download manually to `backend/models/`:
- `Meta-Llama-3-8B-Instruct-Q4_K_M.gguf`
- `inswapper_128.onnx`
- `GFPGANv1.4.pth`

### 4. Run the Application

```bash
# Terminal 1: Backend
cd backend
source venv/bin/activate
uvicorn app:app --host 0.0.0.0 --port 8000

# Terminal 2: Frontend
cd frontend
npm run dev
```

Visit **http://localhost:5173** ğŸ‰

---

## ğŸ”§ Tech Stack

### Backend
- **FastAPI** â€” High-performance Async API
- **Llama.cpp** â€” Local LLM inference
- **InsightFace** â€” State-of-the-art Face Analysis/Swapping
- **GFPGAN** â€” Face Restoration & Upscaling
- **FAISS** â€” Vector Similarity Search
- **PyMuPDF & Tesseract** â€” Document Processing

### Frontend
- **React.js** â€” Modern UI framework
- **Vite** â€” Lightning-fast build tool
- **Lucide React** â€” Beautiful icons
- **Pure CSS** â€” No frameworks, Apple-inspired design

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

---

## ğŸ“„ License

MIT License â€” 100% Free and Open Source

---

<p align="center">
Made by <strong>Zaki Sheriff</strong>
</p>

<p align="center">
<em>Privacy-first AI for everyone.</em>
</p>
